# ğŸš€ N8N Workflow Setup Guide

## ğŸ“‹ Tá»•ng quan

Workflow tá»± Ä‘á»™ng crawl dá»¯ liá»‡u tá»« thuvienphapluat.vn vÃ o Supabase vá»›i 2 nodes chÃ­nh:

1. **Node 1**: Crawl hyperlinks (danh sÃ¡ch URLs)
2. **Node 2**: Crawl document data (metadata + relationships + files)

## ğŸ—ï¸ Kiáº¿n trÃºc

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Schedule Trigger (Every 6h)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NODE 1: Crawl Hyperlinks                           â”‚
â”‚  Command: python n8n_node1_get_urls.py              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Supabase: Insert URLs â†’ doc_urls table             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Supabase: Get Pending URLs (status='pending')      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NODE 2: Crawl Documents                            â”‚
â”‚  Command: python n8n_node2_crawl_docs.py            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Supabase: Insert Data                              â”‚
â”‚  - doc_metadata                                     â”‚
â”‚  - relationships                                    â”‚
â”‚  - doc_files                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Supabase: Update doc_urls.status = 'crawled'      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Prerequisites

### 1. Supabase Setup

Cháº¡y schema SQL:
```bash
# Copy ná»™i dung tá»« supabase_schema.sql
# Paste vÃ o Supabase SQL Editor vÃ  Execute
```

### 2. Login Session

Táº¡o session cookies:
```powershell
python -m tvpl_crawler login-playwright `
  --login-url "https://thuvienphapluat.vn/" `
  --user-selector "#usernameTextBox" `
  --pass-selector "#passwordTextBox" `
  --submit-selector "#loginButton" `
  --cookies-out data\cookies.json `
  --headed
```

### 3. Test Scripts

Test Node 1:
```powershell
python n8n_node1_get_urls.py "https://thuvienphapluat.vn/page/tim-van-ban.aspx?keyword=&match=True&area=0" 2
```

Test Node 2:
```powershell
# Táº¡o file test URLs
echo '["https://thuvienphapluat.vn/van-ban/..."]' > data/test_urls.json

python n8n_node2_crawl_docs.py data/test_urls.json 2
```

## ğŸ”§ N8N Setup

### Option 1: Import Workflow JSON

1. Má»Ÿ n8n
2. Click **Import from File**
3. Chá»n `n8n_complete_workflow.json`
4. Configure Supabase credentials

### Option 2: Manual Setup

#### Node 1: Execute Command
```
Command: python n8n_node1_get_urls.py "https://thuvienphapluat.vn/page/tim-van-ban.aspx?keyword=&match=True&area=0" 5
Working Directory: /app
```

#### Node 2: Execute Command
```
Command: python n8n_node2_crawl_docs.py data/pending_urls.json 2
Working Directory: /app
```

## ğŸ³ Docker Setup

### docker-compose.yml

```yaml
version: "3.8"

services:
  n8n:
    image: docker.n8n.io/n8nio/n8n:latest
    ports:
      - "5678:5678"
    environment:
      - WEBHOOK_URL=http://localhost:5678/
    volumes:
      - n8n_data:/home/node/.n8n
      - ./:/app  # Mount crawler code
    networks:
      - tvpl-net

  crawler:
    build: .
    volumes:
      - ./data:/app/data
    networks:
      - tvpl-net

volumes:
  n8n_data:

networks:
  tvpl-net:
```

### Dockerfile cho crawler

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Install Playwright browsers
RUN playwright install chromium
RUN playwright install-deps chromium

# Copy code
COPY . .

CMD ["tail", "-f", "/dev/null"]
```

## ğŸ“Š Monitoring

### Check Supabase Tables

```sql
-- Tá»•ng sá»‘ URLs
SELECT status, COUNT(*) FROM doc_urls GROUP BY status;

-- Documents crawled hÃ´m nay
SELECT COUNT(*) FROM doc_metadata WHERE created_at > CURRENT_DATE;

-- Relationships
SELECT relationship_type, COUNT(*) FROM relationships GROUP BY relationship_type;

-- Files
SELECT file_type, COUNT(*) FROM doc_files GROUP BY file_type;
```

### N8N Execution Logs

- Xem logs trong n8n UI
- Check file outputs: `data/n8n_*.json`

## ğŸ”„ Workflow Customization

### Thay Ä‘á»•i táº§n suáº¥t crawl

Edit Schedule Trigger:
```json
{
  "interval": [{"field": "hours", "hoursInterval": 6}]
}
```

### Thay Ä‘á»•i sá»‘ URLs crawl má»—i láº§n

Edit "Supabase: Get Pending URLs":
```sql
SELECT id, url FROM doc_urls 
WHERE status = 'pending' 
ORDER BY crawl_priority DESC 
LIMIT 20  -- Thay Ä‘á»•i sá»‘ nÃ y
```

### Thay Ä‘á»•i concurrency

Edit Node 2 command:
```
python n8n_node2_crawl_docs.py data/pending_urls.json 3  # Thay 2 â†’ 3
```

## ğŸ› Troubleshooting

### Session expired

Re-login:
```powershell
python -m tvpl_crawler login-playwright --cookies-out data\cookies.json --headed
```

### CAPTCHA issues

- Giáº£m concurrency: `2 â†’ 1`
- TÄƒng delay trong `crawl_data_fast.py`
- DÃ¹ng `--reuse-session` flag

### Supabase connection errors

Check credentials trong n8n:
- Host
- Database name
- User/Password
- Port (5432)

## ğŸ“ˆ Performance Tips

1. **Batch size**: Crawl 10-20 URLs má»—i láº§n
2. **Concurrency**: Giá»¯ á»Ÿ 2-3 Ä‘á»ƒ trÃ¡nh CAPTCHA
3. **Schedule**: Cháº¡y 4-6h má»™t láº§n
4. **Cleanup**: XÃ³a temp files Ä‘á»‹nh ká»³

## ğŸ” Security

- KhÃ´ng commit `.env` file
- DÃ¹ng Supabase RLS (Row Level Security)
- Rotate credentials Ä‘á»‹nh ká»³
- Monitor failed login attempts

## ğŸ“ Notes

- Node 1 crawl ~100 URLs/5 pages
- Node 2 crawl ~10 docs/batch
- Má»—i doc cÃ³ ~5-15 relationships
- Má»—i doc cÃ³ ~3 download files
