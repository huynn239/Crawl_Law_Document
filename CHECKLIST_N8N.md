# âœ… Checklist - TÃ­ch há»£p N8N vá»›i Supabase

## ğŸ“‹ Tá»•ng Quan

Checklist Ä‘áº§y Ä‘á»§ Ä‘á»ƒ setup vÃ  cháº¡y workflow crawl vÄƒn báº£n lÃªn Supabase qua N8N.

---

## ğŸ”§ PHáº¦N 1: Setup Ban Äáº§u

### âœ… 1.1. CÃ i Ä‘áº·t Dependencies

- [ ] CÃ i Python 3.11+
- [ ] CÃ i dependencies: `pip install -r requirements.txt`
- [ ] CÃ i Playwright: `playwright install chromium`
- [ ] Test Playwright: `python -m playwright --version`

### âœ… 1.2. Setup Supabase

- [ ] Táº¡o project trÃªn Supabase
- [ ] Copy `SUPABASE_URL` vÃ  `SUPABASE_KEY`
- [ ] Cháº¡y `supabase_schema.sql` trong SQL Editor
- [ ] Verify tables Ä‘Ã£ táº¡o:
  ```sql
  SELECT table_name FROM information_schema.tables 
  WHERE table_schema = 'public';
  ```
- [ ] Káº¿t quáº£ pháº£i cÃ³: `doc_urls`, `doc_metadata`, `relationships`, `doc_files`

### âœ… 1.3. Táº¡o file .env

- [ ] Táº¡o file `.env` trong root folder
- [ ] ThÃªm ná»™i dung:
  ```env
  SUPABASE_URL=https://xxx.supabase.co
  SUPABASE_KEY=eyJxxx...
  ```
- [ ] Test connection: `python verify_supabase.py`

### âœ… 1.4. Login Session (Optional)

- [ ] Cháº¡y login script:
  ```powershell
  python -m tvpl_crawler login-playwright `
    --login-url "https://thuvienphapluat.vn/" `
    --user-selector "#usernameTextBox" `
    --pass-selector "#passwordTextBox" `
    --submit-selector "#loginButton" `
    --cookies-out data\cookies.json `
    --headed
  ```
- [ ] Verify file `data/cookies.json` Ä‘Ã£ táº¡o

---

## ğŸ§ª PHáº¦N 2: Test Workflow Thá»§ CÃ´ng

### âœ… 2.1. Test Crawl Links

- [ ] Cháº¡y command:
  ```powershell
  python -m tvpl_crawler links-basic `
    -u "https://thuvienphapluat.vn/page/tim-van-ban.aspx?keyword=&area=0&match=True&type=0&lan=1&status=-1&org=-1&field=-1&year=2025&page={page}" `
    -o data/test_links.json `
    -m 1
  ```
- [ ] Verify file `data/test_links.json` cÃ³ data
- [ ] Check format JSON Ä‘Ãºng:
  ```json
  [
    {
      "Stt": 1,
      "Url": "https://...",
      "Ten van ban": "..."
    }
  ]
  ```

### âœ… 2.2. Test Crawl Documents

- [ ] Cháº¡y command:
  ```powershell
  python crawl_data_fast.py data/test_links.json data/test_result.json
  ```
- [ ] Verify file `data/test_result.json` cÃ³ data
- [ ] Check format cÃ³ Ä‘á»§: `url`, `doc_info`, `tab4`, `tab8`

### âœ… 2.3. Test Transform

- [ ] Cháº¡y command:
  ```powershell
  python supabase_transform.py data/test_result.json
  ```
- [ ] Verify file `data/test_result_supabase.json` Ä‘Ã£ táº¡o
- [ ] Check format cÃ³ Ä‘á»§: `doc_metadata`, `relationships`, `doc_files`
- [ ] Verify `content_hash` Ä‘Ã£ Ä‘Æ°á»£c táº¡o

### âœ… 2.4. Test Import

- [ ] Sá»­a `import_full_supabase.py` Ä‘á»ƒ Ä‘á»c file test:
  ```python
  file_path = Path('data/test_result_supabase.json')
  ```
- [ ] Cháº¡y: `python import_full_supabase.py`
- [ ] Check output:
  ```
  [1/3] Importing metadata...
    Inserted: X, Skipped: 0
  [2/3] Importing relationships...
    Inserted: X
  [3/3] Importing files...
    Inserted: X
  ```
- [ ] Verify data trong Supabase:
  ```sql
  SELECT COUNT(*) FROM doc_urls;
  SELECT COUNT(*) FROM doc_metadata;
  SELECT COUNT(*) FROM relationships;
  SELECT COUNT(*) FROM doc_files;
  ```

### âœ… 2.5. Test Versioning

- [ ] Cháº¡y láº¡i import: `python import_full_supabase.py`
- [ ] Check output:
  ```
  [1/3] Importing metadata...
    Inserted: 0, Skipped: X  # âœ… Pháº£i skip vÃ¬ content_hash giá»‘ng
  ```
- [ ] Verify version trong DB:
  ```sql
  SELECT url, version FROM doc_metadata ORDER BY url, version;
  ```

---

## ğŸš€ PHáº¦N 3: Cháº¡y Workflow Äáº§y Äá»§

### âœ… 3.1. Crawl Batch Äáº§u TiÃªn

- [ ] Cháº¡y crawl links (5-10 pages):
  ```powershell
  python -m tvpl_crawler links-basic `
    -u "https://thuvienphapluat.vn/page/tim-van-ban.aspx?keyword=&area=0&match=True&type=0&lan=1&status=-1&org=-1&field=-1&year=2025&page={page}" `
    -o data/links.json `
    -m 5
  ```
- [ ] Check sá»‘ lÆ°á»£ng URLs: `python -c "import json; print(len(json.load(open('data/links.json'))))"`

### âœ… 3.2. Crawl Documents

- [ ] Cháº¡y crawl documents:
  ```powershell
  python crawl_data_fast.py data/links.json data/result.json
  ```
- [ ] Monitor logs Ä‘á»ƒ check errors
- [ ] Check sá»‘ lÆ°á»£ng documents crawled

### âœ… 3.3. Transform

- [ ] Cháº¡y transform:
  ```powershell
  python supabase_transform.py data/result.json
  ```
- [ ] Verify output file

### âœ… 3.4. Import

- [ ] Sá»­a láº¡i `import_full_supabase.py` Ä‘á»ƒ Ä‘á»c file chÃ­nh:
  ```python
  file_path = Path('data/result_supabase.json')
  ```
- [ ] Cháº¡y import: `python import_full_supabase.py`
- [ ] Monitor logs
- [ ] Verify data trong Supabase

### âœ… 3.5. Verify Káº¿t Quáº£

- [ ] Check tá»•ng sá»‘ documents:
  ```sql
  SELECT COUNT(*) FROM doc_urls WHERE status = 'crawled';
  ```
- [ ] Check relationships:
  ```sql
  SELECT relationship_type, COUNT(*) 
  FROM relationships 
  GROUP BY relationship_type;
  ```
- [ ] Check files:
  ```sql
  SELECT file_type, COUNT(*) 
  FROM doc_files 
  GROUP BY file_type;
  ```
- [ ] Check documents má»›i nháº¥t:
  ```sql
  SELECT * FROM v_document_summary 
  ORDER BY created_at DESC 
  LIMIT 10;
  ```

---

## ğŸ¤– PHáº¦N 4: Setup N8N

### âœ… 4.1. CÃ i Ä‘áº·t N8N

**Option A: Docker**
- [ ] Táº¡o `docker-compose.yml`
- [ ] Cháº¡y: `docker-compose up -d`
- [ ] Access: `http://localhost:5678`

**Option B: NPM**
- [ ] CÃ i: `npm install -g n8n`
- [ ] Cháº¡y: `n8n start`

### âœ… 4.2. Setup Credentials

- [ ] VÃ o N8N â†’ Credentials â†’ Add Credential
- [ ] Chá»n "Supabase"
- [ ] Nháº­p:
  - Host: `https://xxx.supabase.co`
  - Service Role Secret: `eyJxxx...`
- [ ] Test connection

### âœ… 4.3. Import Workflow

**Option A: Import JSON**
- [ ] VÃ o N8N â†’ Import from File
- [ ] Chá»n `n8n_complete_workflow.json`
- [ ] Configure credentials

**Option B: Táº¡o thá»§ cÃ´ng**
- [ ] Táº¡o Schedule Trigger
- [ ] Táº¡o Execute Command nodes
- [ ] Táº¡o Supabase nodes
- [ ] Connect cÃ¡c nodes

### âœ… 4.4. Configure Nodes

**Node 1: Crawl Links**
- [ ] Command: `python n8n_node1_get_urls.py "URL" 5`
- [ ] Working Directory: `/app` hoáº·c project path

**Node 2: Supabase Insert URLs**
- [ ] Table: `doc_urls`
- [ ] Operation: Upsert
- [ ] On Conflict: `url`

**Node 3: Supabase Get Pending URLs**
- [ ] Table: `doc_urls`
- [ ] Operation: Get Many
- [ ] Filters: `status = 'pending'`
- [ ] Limit: 20

**Node 4: Crawl Documents**
- [ ] Command: `python n8n_node2_crawl_docs.py data/pending_urls.json 2`

**Node 5: Supabase Insert Metadata**
- [ ] Table: `doc_metadata`
- [ ] Operation: Insert
- [ ] Map fields tá»« JSON

**Node 6: Supabase Insert Relationships**
- [ ] Table: `relationships`
- [ ] Operation: Insert

**Node 7: Supabase Insert Files**
- [ ] Table: `doc_files`
- [ ] Operation: Insert

### âœ… 4.5. Test Workflow

- [ ] Click "Execute Workflow"
- [ ] Check logs tá»«ng node
- [ ] Verify data trong Supabase
- [ ] Fix errors náº¿u cÃ³

---

## ğŸ“Š PHáº¦N 5: Monitoring & Maintenance

### âœ… 5.1. Setup Monitoring

- [ ] Táº¡o dashboard trong Supabase
- [ ] Monitor queries:
  ```sql
  -- Pending crawls
  SELECT COUNT(*) FROM doc_urls WHERE status = 'pending';
  
  -- Crawled today
  SELECT COUNT(*) FROM doc_metadata WHERE created_at > CURRENT_DATE;
  
  -- Errors
  SELECT COUNT(*) FROM doc_urls WHERE status = 'error';
  ```

### âœ… 5.2. Schedule Regular Crawls

- [ ] Set schedule trong N8N (e.g., má»—i 6h)
- [ ] Monitor execution logs
- [ ] Check for failures

### âœ… 5.3. Backup

- [ ] Setup automatic backups trong Supabase
- [ ] Backup local data files Ä‘á»‹nh ká»³
- [ ] Test restore process

### âœ… 5.4. Cleanup

- [ ] XÃ³a temp files: `data/temp_*.json`
- [ ] Archive old data
- [ ] Monitor disk space

---

## ğŸ› PHáº¦N 6: Troubleshooting

### âœ… 6.1. Common Issues

**Session expired:**
- [ ] Re-login: `python -m tvpl_crawler login-playwright ...`

**CAPTCHA issues:**
- [ ] Giáº£m concurrency: `2 â†’ 1`
- [ ] TÄƒng delay
- [ ] DÃ¹ng `--reuse-session`

**Supabase connection errors:**
- [ ] Check credentials
- [ ] Check network
- [ ] Check Supabase status

**Duplicate key violations:**
- [ ] Normal! Trigger sáº½ handle
- [ ] Check logs Ä‘á»ƒ verify

### âœ… 6.2. Debug Commands

```powershell
# Check database
python check_database.py

# Verify Supabase
python verify_supabase.py

# Test single URL
python run_single_url.py "https://..."

# Check downloads
python check_downloads.py
```

---

## ğŸ¯ PHáº¦N 7: Optimization

### âœ… 7.1. Performance Tuning

- [ ] Adjust concurrency (2-3 optimal)
- [ ] Batch size (10-20 URLs)
- [ ] Schedule frequency (4-6h)

### âœ… 7.2. Database Optimization

- [ ] Check indexes:
  ```sql
  SELECT * FROM pg_indexes WHERE tablename IN ('doc_urls', 'doc_metadata');
  ```
- [ ] Analyze query performance
- [ ] Vacuum database Ä‘á»‹nh ká»³

### âœ… 7.3. Code Optimization

- [ ] Profile slow functions
- [ ] Optimize queries
- [ ] Cache frequently accessed data

---

## ğŸ‰ HoÃ n ThÃ nh!

Sau khi hoÃ n thÃ nh checklist nÃ y, báº¡n sáº½ cÃ³:

âœ… Workflow tá»± Ä‘á»™ng crawl vÄƒn báº£n
âœ… Data Ä‘Æ°á»£c lÆ°u trong Supabase vá»›i versioning
âœ… N8N workflow cháº¡y tá»± Ä‘á»™ng theo schedule
âœ… Monitoring vÃ  alerting
âœ… Backup vÃ  recovery plan

**Next Steps:**
- [ ] Äá»c [WORKFLOW_COMPLETE.md](WORKFLOW_COMPLETE.md) Ä‘á»ƒ hiá»ƒu chi tiáº¿t
- [ ] Äá»c [IMPORT_FIX_SUMMARY.md](IMPORT_FIX_SUMMARY.md) Ä‘á»ƒ hiá»ƒu cÃ¡c fix
- [ ] Äá»c [README_N8N.md](README_N8N.md) Ä‘á»ƒ setup N8N

---

## ğŸ“ Support

Náº¿u gáº·p váº¥n Ä‘á»:
1. Check logs trong N8N
2. Check Supabase logs
3. Check file documentation
4. Create GitHub issue

Good luck! ğŸš€
