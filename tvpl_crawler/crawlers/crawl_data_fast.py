# -*- coding: utf-8 -*-
"""Crawl nhanh v·ªõi async/concurrent v√† reuse browser context (B·∫£n n√¢ng c·∫•p ch·ªëng CAPTCHA)"""
import json
import sys
import os
import asyncio
import random
from pathlib import Path
from dotenv import load_dotenv
from playwright.async_api import async_playwright
from tvpl_crawler.compact_schema import compact_schema
from tvpl_crawler.core.db import TVPLDatabase
from tvpl_crawler.playwright_extract_async import extract_luoc_do_async
from tvpl_crawler.utils.captcha_solver import bypass_captcha

try:
    from playwright_stealth import stealth_async
    STEALTH_AVAILABLE = True
    print("‚úì playwright-stealth available")
except ImportError:
    STEALTH_AVAILABLE = False
    print("‚ö† playwright-stealth NOT installed - Cloudflare may detect bot")
    print("  Install: pip install playwright-stealth")

load_dotenv()

# Danh s√°ch User-Agent ph·ªï bi·∫øn ƒë·ªÉ xoay v√≤ng
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/118.0.2088.76",
]

if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

if len(sys.argv) < 2:
    print("Usage: python crawl_data_fast.py <input_file> [output_file] [concurrency] [timeout_ms] [--reuse-session] [--headed] [--save-per-batch]")
    print("Example: python crawl_data_fast.py data/links.json data/result.json 2 30000")
    print("         python crawl_data_fast.py data/links.json data/result.json 2 30000 --reuse-session")
    print("         python crawl_data_fast.py data/links.json data/result.json 2 30000 --headed")
    print("Default: concurrency=2, timeout=30000ms (30s)")
    print("--reuse-session: D√πng session c√≥ s·∫µn (kh√¥ng login l·∫°i m·ªói batch)")
    print("--headed: Hi·ªÉn th·ªã browser (debug)")
    print("--save-per-batch: L∆∞u v√†o Supabase sau m·ªói batch (kh√¥ng c·∫ßn import cu·ªëi)")
    sys.exit(1)

# Parse arguments
REUSE_SESSION = "--reuse-session" in sys.argv
HEADED = "--headed" in sys.argv
SAVE_PER_BATCH = "--save-per-batch" in sys.argv
args = [arg for arg in sys.argv[1:] if not arg.startswith("--")]

input_file = Path(args[0])
output_file = Path(args[1]) if len(args) >= 2 else input_file.parent / f"{input_file.stem}_Result.json"
CONCURRENCY = int(args[2]) if len(args) >= 3 else 2
TIMEOUT_MS = int(args[3]) if len(args) >= 4 else 30000

links = json.loads(input_file.read_text(encoding="utf-8"))
print(f"Crawl {len(links)} vƒÉn b·∫£n (concurrency={CONCURRENCY}, timeout={TIMEOUT_MS//1000}s)\n")

async def relogin(browser, storage_file="data/storage_state.json", user_agent=None, viewport=None):
    """Login v·ªõi session m·ªõi"""
    for attempt in range(2):
        context = None
        page = None
        try:
            context_options = {}
            if user_agent:
                context_options["user_agent"] = user_agent
            if viewport:
                context_options["viewport"] = viewport
            
            context = await browser.new_context(**context_options)
            page = await context.new_page()
            if STEALTH_AVAILABLE:
                await stealth_async(page)
            page.set_default_timeout(60000)  # 60s timeout
            await page.goto("https://thuvienphapluat.vn/", wait_until="domcontentloaded")
            
            # ƒê·ª£i Cloudflare pass (n·∫øu c√≥)
            await page.wait_for_timeout(3000)
            try:
                cloudflare = await page.query_selector("text=Verify you are human")
                if cloudflare:
                    print("  üîí Cloudflare at login, waiting...")
                    for i in range(20):
                        await page.wait_for_timeout(1000)
                        cloudflare = await page.query_selector("text=Verify you are human")
                        if not cloudflare:
                            print("  ‚úì Cloudflare passed")
                            break
                    else:
                        raise Exception("Cloudflare timeout at login")
            except Exception as e:
                if "Cloudflare" in str(e):
                    raise
            
            break
        except Exception as e:
            if page:
                try: await page.close()
                except: pass
            if context:
                try: await context.close()
                except: pass
            
            if attempt < 1:
                print(f"  ‚ö† Login timeout, retry sau 5s...")
                await asyncio.sleep(5)
                continue
            raise e
    
    # X·ª≠ l√Ω popup consent (th·ª≠ nhi·ªÅu selector)
    try:
        # Th·ª≠ click n√∫t "Consent" ho·∫∑c "Do not consent"
        consent_selectors = [
            "button:has-text('Consent')",
            "button:has-text('Do not consent')",
            ".fc-cta-consent",
            "button.fc-button.fc-cta-consent",
            "[aria-label='Consent']"
        ]
        for selector in consent_selectors:
            try:
                await page.click(selector, timeout=2000)
                await page.wait_for_timeout(1000)
                break
            except:
                continue
    except:
        pass
    
    await page.fill("#usernameTextBox", os.getenv("TVPL_USERNAME", ""))
    await page.wait_for_timeout(random.uniform(500, 1500))  # Delay sau khi nh·∫≠p username
    await page.fill("#passwordTextBox", os.getenv("TVPL_PASSWORD", ""))
    await page.wait_for_timeout(random.uniform(300, 800))  # Delay tr∆∞·ªõc khi click login
    await page.click("#loginButton")
    await page.wait_for_timeout(3000)
    
    try:
        btn_count = await page.evaluate("""() => { return document.querySelectorAll('div.ui-dialog-buttonpane button').length; }""")
        if btn_count > 0:
            await page.keyboard.press("Enter")
            await page.wait_for_timeout(500)
            await page.evaluate("""() => { const btn = document.querySelector('div.ui-dialog-buttonpane button'); if (btn) { btn.dispatchEvent(new MouseEvent('click', {bubbles: true, cancelable: true})); } }""")
            await page.wait_for_timeout(500)
    except:
        pass
    
    # X·ª≠ l√Ω CAPTCHA n·∫øu c√≥
    if not await bypass_captcha(page, doc_id=0):
        print("‚ö†Ô∏è Kh√¥ng th·ªÉ bypass CAPTCHA khi login")
        await context.close()
        return None
    
    await context.storage_state(path=storage_file)
    await context.close()
    return storage_file

async def check_login_required(page):
    """Ki·ªÉm tra xem c√≥ c·∫ßn login kh√¥ng"""
    try:
        login_form = await page.query_selector("#usernameTextBox")
        return login_form is not None
    except:
        return False

def _has_valid_doc_info(doc_info):
    """Check if doc_info has valid data (not empty or 'D·ªØ li·ªáu ƒëang c·∫≠p nh·∫≠t')"""
    return any(v for v in doc_info.values() if v and v != "D·ªØ li·ªáu ƒëang c·∫≠p nh·∫≠t")

async def crawl_one_with_context(context, item, semaphore, timeout_ms):
    """Crawl m·ªôt vƒÉn b·∫£n - reuse context trong batch"""
    async with semaphore:
        stt = item.get("Stt") or item.get("stt") or 0
        url = item.get("Url") or item.get("url") or ""
        ten = item.get("Ten van ban") or item.get("ten_van_ban") or ""
        ngay_cap_nhat = item.get("Ngay cap nhat") or item.get("ngay_cap_nhat") or ""
        
        if not url:
            print(f"\n[{stt}] ‚úó Missing URL")
            return {"stt": stt, "error": "Missing URL", "doc_info": {}}
        
        print(f"\n[{stt}/{len(links)}] {ten[:60] if ten else 'No title'}...")
        print(f"  URL: {url}")

        # Th√™m ƒë·ªô tr·ªÖ ng·∫´u nhi√™n tr∆∞·ªõc m·ªói request
        delay = random.uniform(5.0, 8.0)
        print(f"  ‚è± Delay {delay:.1f}s...")
        await asyncio.sleep(delay)
        
        for attempt in range(3):
            page = None
            try:
                page = await context.new_page()
                if STEALTH_AVAILABLE:
                    await stealth_async(page)
                current_timeout = timeout_ms * (1 + attempt)
                data = await extract_luoc_do_async(url, page, timeout_ms=current_timeout)
                data["stt"] = stt
                data["title"] = ten
                if ngay_cap_nhat:
                    data["ngay_cap_nhat"] = ngay_cap_nhat
                
                total = data.get("tab4_total_relations", 0)
                doc_info = data.get("doc_info", {})
                has_valid_data = _has_valid_doc_info(doc_info)
                
                # Check session ch·ªâ khi data null
                if not has_valid_data:
                    if await check_login_required(page):
                        await page.close()
                        raise Exception("Session expired - login required")
                
                # Close page
                await page.close()
                
                if not has_valid_data and attempt < 2:
                    delay = random.uniform(20, 25)
                    print(f"  ‚ö† [{stt}] Data null (keys: {list(doc_info.keys())}), retry {attempt+1}/3 sau {delay:.1f}s...")
                    await asyncio.sleep(delay)
                    continue
                
                # Skip n·∫øu v·∫´n null sau 3 retry
                if not has_valid_data:
                    print(f"  ‚ö† [{stt}] Data null sau 3 retry (keys: {list(doc_info.keys())}), skip")
                    return {"stt": stt, "url": url, "error": "No valid data after 3 retries", "doc_info": doc_info}
                
                print(f"  ‚úì [{stt}] {total} quan h·ªá")
                return data
            except Exception as e:
                error_msg = str(e)
                if page:
                    try:
                        await page.close()
                    except:
                        pass
                
                # Log chi ti·∫øt h∆°n cho c√°c lo·∫°i l·ªói
                if "CAPTCHA" in error_msg:
                    print(f"  üîí [{stt}] CAPTCHA kh√¥ng bypass ƒë∆∞·ª£c")
                elif "Timeout" in error_msg:
                    if attempt < 2:
                        delay = random.uniform(20, 25)
                        print(f"  ‚ö† [{stt}] Timeout, retry {attempt+1}/3 sau {delay:.1f}s...")
                        await asyncio.sleep(delay)
                        continue
                    print(f"  ‚ö† [{stt}] Timeout sau 3 l·∫ßn th·ª≠")
                else:
                    print(f"  ‚úó [{stt}] {error_msg[:80]}")
                
                return {"stt": stt, "url": url, "error": error_msg, "doc_info": {}}
            
        return {"stt": stt, "url": url, "error": "Failed after retry", "doc_info": {}}

async def crawl_one(browser, item, semaphore, storage_path, timeout_ms):
    """Crawl m·ªôt vƒÉn b·∫£n - t·∫°o context ri√™ng (d√πng cho retry khi session l·ªói)"""
    async with semaphore:
        stt = item.get("Stt") or item.get("stt") or 0
        url = item.get("Url") or item.get("url") or ""
        ten = item.get("Ten van ban") or item.get("ten_van_ban") or ""
        ngay_cap_nhat = item.get("Ngay cap nhat") or item.get("ngay_cap_nhat") or ""
        
        if not url:
            return {"stt": stt, "error": "Missing URL", "doc_info": {}}
        
        print(f"[{stt}/{len(links)}] {ten[:60] if ten else 'No title'}... (RETRY)")
        
        for attempt in range(3):
            context = None
            page = None
            try:
                # D√πng storage_state ·ªü ƒë√¢y v√¨ ch√∫ng ta v·ª´a login l·∫°i
                context = await browser.new_context(storage_state=str(storage_path))
                page = await context.new_page()
                if STEALTH_AVAILABLE:
                    await stealth_async(page)
                
                current_timeout = timeout_ms * (1 + attempt)
                
                data = await extract_luoc_do_async(url, page, timeout_ms=current_timeout)
                data["stt"] = stt
                data["title"] = ten
                if ngay_cap_nhat:
                    data["ngay_cap_nhat"] = ngay_cap_nhat
                
                await page.close()
                await context.close()
                
                total = data.get("tab4_total_relations", 0)
                doc_info = data.get("doc_info", {})
                has_valid_data = _has_valid_doc_info(doc_info)
                
                if not has_valid_data and attempt < 2:
                    delay = random.uniform(20, 25)
                    print(f"  ‚ö† [{stt}] Data null, retry {attempt+1}/3 sau {delay:.1f}s...")
                    await asyncio.sleep(delay)
                    continue
                
                # Skip n·∫øu v·∫´n null sau 3 retry
                if not has_valid_data:
                    print(f"  ‚ö† [{stt}] Data null sau 3 retry, skip")
                    return {"stt": stt, "url": url, "error": "No valid data after 3 retries", "doc_info": {}}
                
                print(f"  ‚úì [{stt}] {total} quan h·ªá")
                return data
            except Exception as e:
                error_msg = str(e)
                if page:
                    try: await page.close()
                    except: pass
                if context:
                    try: await context.close()
                    except: pass
                
                if "Timeout" in error_msg and attempt < 2:
                    delay = random.uniform(20, 25)
                    print(f"  ‚ö† [{stt}] Timeout, retry {attempt+1}/3 sau {delay:.1f}s...")
                    await asyncio.sleep(delay)
                    continue
                
                print(f"  ‚úó [{stt}] {error_msg[:80]}")
                return {"stt": stt, "url": url, "error": error_msg, "doc_info": {}}
        
        return {"stt": stt, "url": url, "error": "Failed after retry", "doc_info": {}}

async def main():
    global SAVE_PER_BATCH
    
    # Start single session for entire crawl
    global_session_id = None
    if SAVE_PER_BATCH and os.getenv('SUPABASE_URL'):
        try:
            from tvpl_crawler.import_supabase_v2 import start_session
            global_session_id = start_session()
            print(f"‚úì Started crawl session #{global_session_id}\n")
        except Exception as e:
            print(f"‚ö† Could not connect to Supabase: {str(e)[:100]}")
            print("‚ö† Will save to JSON only (no DB)\n")
            SAVE_PER_BATCH = False
    
    try:
        return await _run_crawl(global_session_id)
    except KeyboardInterrupt:
        if global_session_id:
            try:
                _fail_session(global_session_id, "Cancelled by user")
            except:
                pass
        raise
    except Exception as e:
        if global_session_id:
            try:
                _fail_session(global_session_id, str(e))
            except:
                pass
        raise

def _fail_session(session_id, error_msg):
    try:
        from tvpl_crawler.import_supabase_v2 import supabase
        from datetime import datetime
        supabase.table('crawl_sessions').update({
            'status': 'FAILED',
            'completed_at': datetime.now().isoformat()
        }).eq('session_id', session_id).execute()
        print(f"\n‚úó Session #{session_id} marked as FAILED: {error_msg[:100]}")
    except Exception as e:
        print(f"\n‚ö† Could not update session: {e}")

async def _run_crawl(global_session_id):
    async with async_playwright() as p:
        # D√πng Chrome th·∫≠t ƒë·ªÉ bypass Cloudflare (t·ªët h∆°n Chromium)
        try:
            browser = await p.chromium.launch(
                channel="chrome",
                headless=not HEADED,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-gpu' if not HEADED else '',  # T·∫Øt GPU check khi headless
                    '--window-size=1920,1080'
                ]
            )
        except:
            print(" using Chromium")
            browser = await p.chromium.launch(
                headless=not HEADED,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-gpu' if not HEADED else '',
                    '--window-size=1920,1080'
                ]
            )
        
        semaphore = asyncio.Semaphore(CONCURRENCY)
        
        BATCH_SIZE = 10  # Gi·∫£m xu·ªëng 5 ƒë·ªÉ tr√°nh Cloudflare
        results = []
        total_new = 0
        total_unchanged = 0
        
        # Login 1 l·∫ßn ·ªü ƒë·∫ßu
        storage_path = Path("data/storage_state.json")
        print("üîê ƒêang login...")
        if not await relogin(browser, str(storage_path)):
            print("‚ö†Ô∏è Login th·∫•t b·∫°i")
            await browser.close()
            return []
        print("‚úì Login th√†nh c√¥ng\n")
        
        # Random User-Agent v√† Viewport
        user_agent = random.choice(USER_AGENTS)
        viewport = {"width": random.randint(1280, 1920), "height": random.randint(720, 1080)}
        
        # T·∫°o context 1 l·∫ßn cho to√†n b·ªô crawl
        context = await browser.new_context(
            storage_state=str(storage_path),
            user_agent=user_agent,
            viewport=viewport,
            ignore_https_errors=True,
        )
        await context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            window.chrome = {runtime: {}};
        """)
        
        for batch_idx in range(0, len(links), BATCH_SIZE):
            batch = links[batch_idx:batch_idx + BATCH_SIZE]
            batch_num = batch_idx // BATCH_SIZE + 1
            total_batches = (len(links) - 1) // BATCH_SIZE + 1
            
            print(f"\n{'='*60}")
            print(f"Batch {batch_num}/{total_batches} - {len(batch)} vƒÉn b·∫£n")
            print(f"{'='*60}")
            
            tasks = [crawl_one_with_context(context, item, semaphore, TIMEOUT_MS) for item in batch]
            batch_results = await asyncio.gather(*tasks)
            results.extend(batch_results)
            
            # L∆∞u batch v√†o Supabase ngay (n·∫øu c√≥ flag)
            if SAVE_PER_BATCH and global_session_id:
                try:
                    from tvpl_crawler.import_supabase_v2 import save_document
                    compact_batch = compact_schema(batch_results)
                    batch_new = 0
                    batch_unchanged = 0
                    for item in compact_batch:
                        if item.get("error"):
                            continue
                        try:
                            if save_document(item, global_session_id):
                                batch_new += 1
                            else:
                                batch_unchanged += 1
                        except Exception as e:
                            print(f"  ‚úó L·ªói l∆∞u Supabase: {e}")
                    total_new += batch_new
                    total_unchanged += batch_unchanged
                    print(f"  üíæ ƒê√£ l∆∞u batch: {batch_new} changed, {batch_unchanged} unchanged")
                except Exception as e:
                    print(f"  ‚ö† Kh√¥ng th·ªÉ l∆∞u batch: {e}")
            
            # Delay gi·ªØa c√°c batch
            if batch_idx + BATCH_SIZE < len(links):
                delay = random.uniform(20, 30)
                print(f"\n‚è∏  ƒê·ª£i {delay:.1f}s tr∆∞·ªõc batch ti·∫øp theo...")
                await asyncio.sleep(delay)
        
        await context.close()
        
        # X·ª≠ l√Ω c√°c vƒÉn b·∫£n b·ªã l·ªói session expired
        failed_items = [(i, r) for i, r in enumerate(results) if "Session expired" in r.get("error", "")]
        if failed_items:
            print(f"\n{'='*60}")
            print(f"‚ö†Ô∏è  {len(failed_items)} vƒÉn b·∫£n l·ªói session expired")
            print(f"üîê ƒêang login l·∫°i...")
            print(f"{'='*60}")
            
            if await relogin(browser, str(storage_path)):
                print("‚úì Login l·∫°i th√†nh c√¥ng\n")
                
                # T·∫°o context m·ªõi v·ªõi session m·ªõi
                retry_context = await browser.new_context(
                    storage_state=str(storage_path),
                    user_agent=user_agent,
                    viewport=viewport,
                    ignore_https_errors=True,
                )
                await retry_context.add_init_script("""
                    Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
                    window.chrome = {runtime: {}};
                """)
                
                # Retry t·ª´ng vƒÉn b·∫£n b·ªã l·ªói
                print(f"üîÑ ƒêang retry {len(failed_items)} vƒÉn b·∫£n...\n")
                retry_tasks = [crawl_one_with_context(retry_context, links[i], semaphore, TIMEOUT_MS) for i, _ in failed_items]
                retry_results = await asyncio.gather(*retry_tasks)
                
                # C·∫≠p nh·∫≠t k·∫øt qu·∫£
                for (idx, _), new_result in zip(failed_items, retry_results):
                    results[idx] = new_result
                
                # L∆∞u retry results v√†o Supabase n·∫øu c·∫ßn
                if SAVE_PER_BATCH and global_session_id:
                    try:
                        from tvpl_crawler.import_supabase_v2 import save_document
                        compact_retry = compact_schema(retry_results)
                        retry_new = 0
                        retry_unchanged = 0
                        for item in compact_retry:
                            if item.get("error"):
                                continue
                            try:
                                if save_document(item, global_session_id):
                                    retry_new += 1
                                else:
                                    retry_unchanged += 1
                            except Exception as e:
                                print(f"  ‚úó L·ªói l∆∞u retry: {e}")
                        total_new += retry_new
                        total_unchanged += retry_unchanged
                        print(f"  üíæ ƒê√£ l∆∞u retry: {retry_new} changed, {retry_unchanged} unchanged")
                    except Exception as e:
                        print(f"  ‚ö† Kh√¥ng th·ªÉ l∆∞u retry: {e}")
                
                await retry_context.close()
                print(f"\n‚úì Retry xong {len(failed_items)} vƒÉn b·∫£n")
            else:
                print("‚úó Login l·∫°i th·∫•t b·∫°i, b·ªè qua retry")
        
        await browser.close()
    
        # Complete session
        if global_session_id:
            try:
                from tvpl_crawler.import_supabase_v2 import complete_session
                complete_session(global_session_id, len(results), total_new, total_unchanged)
                print(f"\n‚úì Completed session #{global_session_id}: {total_new} changed, {total_unchanged} unchanged")
            except Exception as e:
                print(f"\n‚ö† Could not complete session: {str(e)[:100]}")
        
        return results

# Ch·∫°y crawler
results = asyncio.run(main())

# L∆∞u JSON tr∆∞·ªõc (backup n·∫øu DB fail)
compact_results = compact_schema(results)
try:
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(compact_results, f, ensure_ascii=False, indent=2)
except (IOError, OSError, PermissionError) as e:
    print(f"‚úó L·ªói l∆∞u JSON ({type(e).__name__}): {e}")
    backup_file = output_file.parent / f"{output_file.stem}_backup.json"
    with open(backup_file, "w", encoding="utf-8") as f:
        json.dump(compact_results, f, ensure_ascii=False, indent=2)
    print(f"‚úì ƒê√£ l∆∞u backup: {backup_file}")
except Exception as e:
    print(f"‚úó L·ªói kh√¥ng x√°c ƒë·ªãnh ({type(e).__name__}): {e}")
    backup_file = output_file.parent / f"{output_file.stem}_backup.json"
    with open(backup_file, "w", encoding="utf-8") as f:
        json.dump(compact_results, f, ensure_ascii=False, indent=2)
    print(f"‚úì ƒê√£ l∆∞u backup: {backup_file}")

# Note: Database saving is handled by --save-per-batch flag (saves to Supabase per batch)
# Final JSON is kept as backup

print(f"\n{'='*60}")
print(f"Crawl xong {len(results)}/{len(links)} vƒÉn b·∫£n")
print(f"K·∫øt qu·∫£: {output_file}")
print(f"{'='*60}")