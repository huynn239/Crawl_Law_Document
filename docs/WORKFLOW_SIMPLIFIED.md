# ğŸ”„ N8N Workflow - Simplified & Correct

## ğŸ“‹ Flow ChÃ­nh XÃ¡c

```
[Schedule: Every 6h]
    â†“
[Node 1: Crawl Hyperlinks]
    Command: python n8n_node1_get_urls.py "..." 5
    Output: links.json (cÃ³ ngay_cap_nhat)
    â†“
[Supabase: Insert URLs]
    INSERT INTO doc_urls (url, status='pending')
    ON CONFLICT (url) DO NOTHING
    â†“
[Supabase: Get Pending URLs]
    SELECT * FROM doc_urls 
    WHERE status='pending' 
    LIMIT 10
    â†“
[Node 2: Crawl Documents]
    Command: python n8n_node2_crawl_docs.py urls.json 2
    Output: documents.json (cÃ³ metadata + relationships + files)
    â†“
[Code: Check Each Document]
    FOR EACH document:
      latest = query_latest_version(url)
      
      IF latest is NULL:
        action = "insert_new"
      ELSE IF doc.ngay_cap_nhat > latest.ngay_cap_nhat:
        action = "insert_version"
      ELSE IF doc.content_hash != latest.content_hash:
        action = "insert_version"
      ELSE:
        action = "skip"
    â†“
[Switch: Route by Action]
    â”œâ”€ insert_new â†’ [Insert All Data]
    â”œâ”€ insert_version â†’ [Insert All Data]
    â””â”€ skip â†’ [Update Timestamp Only]
    â†“
[Supabase: Update Status]
    UPDATE doc_urls SET status='crawled'
```

## ğŸ¯ Key Points

### 1. Insert URLs - KHÃ”NG duplicate
```sql
INSERT INTO doc_urls (url, status)
VALUES ('https://...', 'pending')
ON CONFLICT (url) DO NOTHING;
-- Náº¿u URL Ä‘Ã£ tá»“n táº¡i â†’ SKIP
```

### 2. Crawl Batch - 10 URLs/láº§n
```sql
SELECT id, url FROM doc_urls
WHERE status = 'pending'
ORDER BY crawl_priority DESC
LIMIT 10;
```

### 3. Check Changes - CHá»ˆ insert khi cÃ³ thay Ä‘á»•i
```javascript
// Pseudo-code
if (isChanged) {
  // INSERT version má»›i
  // INSERT relationships má»›i
  // INSERT files má»›i
} else {
  // SKIP - KhÃ´ng insert gÃ¬
  // Chá»‰ update timestamp
}
```

### 4. Version Management - Tá»± Ä‘á»™ng
```sql
-- Trigger tá»± Ä‘á»™ng tÄƒng version
CREATE TRIGGER auto_increment_version
BEFORE INSERT ON doc_metadata
FOR EACH ROW
EXECUTE FUNCTION auto_increment_version();
```

## ğŸ“Š N8N Nodes Detail

### Node: Check Document Changed

```javascript
const item = $input.item.json;
const url = item.url;
const ngay_cap_nhat = item.ngay_cap_nhat;
const content_hash = item.content_hash;

// Query latest version
const query = `
  SELECT ngay_cap_nhat, content_hash, version
  FROM doc_metadata dm
  JOIN doc_urls du ON dm.doc_url_id = du.id
  WHERE du.url = $1
  ORDER BY dm.version DESC
  LIMIT 1
`;

const result = await $supabase.query(query, [url]);

let action = 'skip';

if (result.length === 0) {
  // Láº§n Ä‘áº§u
  action = 'insert_new';
} else {
  const latest = result[0];
  
  // Check thay Ä‘á»•i
  if (ngay_cap_nhat > latest.ngay_cap_nhat || 
      content_hash !== latest.content_hash) {
    action = 'insert_version';
  }
}

return {
  json: {
    ...item,
    action: action,
    should_insert: action !== 'skip'
  }
};
```

### Node: Insert All Data (Khi cÃ³ thay Ä‘á»•i)

```javascript
// Chá»‰ cháº¡y khi should_insert = true

// 1. Insert metadata
await $supabase.insert('doc_metadata', {
  url: item.url,
  so_hieu: item.so_hieu,
  // ... other fields
  ngay_cap_nhat: item.ngay_cap_nhat,
  content_hash: item.content_hash
  // version sáº½ tá»± Ä‘á»™ng tÄƒng bá»Ÿi trigger
});

// 2. Insert relationships
for (const rel of item.relationships) {
  await $supabase.insert('relationships', rel);
}

// 3. Insert files
for (const file of item.files) {
  await $supabase.insert('doc_files', file);
}
```

### Node: Update Timestamp Only (Khi KHÃ”NG thay Ä‘á»•i)

```javascript
// Chá»‰ cháº¡y khi should_insert = false

await $supabase.update('doc_urls', {
  where: { url: item.url },
  data: { updated_at: 'NOW()' }
});
```

## ğŸ”¢ Example Scenarios

### Scenario A: 100 URLs má»›i
```
Crawl hyperlinks â†’ 100 URLs
Insert to doc_urls â†’ 100 rows (status='pending')
Get pending â†’ 10 URLs (batch 1)
Crawl documents â†’ 10 docs
Check changes â†’ ALL are new (action='insert_new')
Insert data â†’ 10 metadata + relationships + files
Update status â†’ 10 URLs (status='crawled')

Repeat 9 more times for remaining 90 URLs
```

### Scenario B: Re-crawl 100 URLs cÅ©
```
Crawl hyperlinks â†’ 100 URLs
Insert to doc_urls â†’ 0 rows (ON CONFLICT DO NOTHING)
Get pending â†’ 0 URLs (all are 'crawled')

Manual trigger re-crawl:
  Update status â†’ 100 URLs (status='pending')
  Get pending â†’ 10 URLs (batch 1)
  Crawl documents â†’ 10 docs
  Check changes:
    - 8 docs: KHÃ”NG Ä‘á»•i (action='skip')
    - 2 docs: CÃ“ Ä‘á»•i (action='insert_version')
  Insert data â†’ 2 metadata + relationships + files
  Update status â†’ 10 URLs (status='crawled')
```

## ğŸ“ˆ Performance

- **Batch size**: 10 URLs/láº§n
- **Concurrency**: 2 (trong crawl_data_fast.py)
- **Time per batch**: ~2-3 phÃºt
- **Total time for 100 URLs**: ~20-30 phÃºt

## âš ï¸ Important

1. **ON CONFLICT DO NOTHING**: KhÃ´ng duplicate URLs
2. **Check before insert**: Chá»‰ insert khi cÃ³ thay Ä‘á»•i
3. **Batch processing**: 10 URLs/láº§n Ä‘á»ƒ trÃ¡nh timeout
4. **Auto-increment version**: Trigger tá»± Ä‘á»™ng handle
5. **Update timestamp**: LuÃ´n update Ä‘á»ƒ track láº§n crawl cuá»‘i
