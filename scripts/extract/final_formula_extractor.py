#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Final Formula Extractor - H·ªá th·ªëng tr√≠ch xu·∫•t c√¥ng th·ª©c cu·ªëi c√πng v·ªõi kh·∫£ nƒÉng x·ª≠ l√Ω ƒëa d·∫°ng"""
import re
import json
import asyncio
from typing import Dict, List, Optional
from pathlib import Path
from playwright.async_api import async_playwright
import sys

if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

class FinalFormulaExtractor:
    def __init__(self):
        # Patterns m·ªü r·ªông cho nhi·ªÅu lo·∫°i c√¥ng th·ª©c
        self.patterns = [
            # 1. M·ª©c ti·ªÅn c·ª• th·ªÉ
            {
                'name': 'M·ª©c ti·ªÅn c·ª• th·ªÉ',
                'pattern': r'(m·ª©c\s+[^=:]{5,60})\s*[=:]\s*([\d.,]+(?:\s*(?:ƒë·ªìng|vnd|tri·ªáu|t·ª∑))?(?:/(?:th√°ng|nƒÉm|ng√†y))?)',
                'confidence': 0.95,
                'type': 'amount_definition'
            },
            
            # 2. T·ª∑ l·ªá ph·∫ßn trƒÉm
            {
                'name': 'T·ª∑ l·ªá ph·∫ßn trƒÉm',
                'pattern': r'(t·ª∑\s*l·ªá\s+[^:=]{5,60})\s*[=:]\s*([\d.,]+\s*(?:%|ph·∫ßn\s*trƒÉm))',
                'confidence': 0.9,
                'type': 'percentage_rate'
            },
            
            # 3. Thu·∫ø su·∫•t
            {
                'name': 'Thu·∫ø su·∫•t',
                'pattern': r'(thu·∫ø\s*su·∫•t[^:=]{0,40})\s*[=:]\s*([\d.,]+\s*%)',
                'confidence': 0.95,
                'type': 'tax_rate'
            },
            
            # 4. C√¥ng th·ª©c t√≠nh thu·∫ø
            {
                'name': 'C√¥ng th·ª©c t√≠nh thu·∫ø',
                'pattern': r'(thu·∫ø[^=]{5,50})\s*=\s*([^.]{10,100}(?:√ó|x|\*)[^.]{5,50})',
                'confidence': 0.9,
                'type': 'tax_calculation'
            },
            
            # 5. L∆∞∆°ng c∆° b·∫£n/t·ªëi thi·ªÉu
            {
                'name': 'L∆∞∆°ng c∆° b·∫£n',
                'pattern': r'(l∆∞∆°ng\s*(?:c∆°\s*b·∫£n|t·ªëi\s*thi·ªÉu|c∆°\s*s·ªü)[^=:]{0,40})\s*[=:]\s*([\d.,]+\s*(?:ƒë·ªìng|vnd)(?:/(?:th√°ng|nƒÉm))?)',
                'confidence': 0.95,
                'type': 'salary_base'
            },
            
            # 6. Ph·ª• c·∫•p
            {
                'name': 'Ph·ª• c·∫•p',
                'pattern': r'(ph·ª•\s*c·∫•p[^=:]{5,50})\s*[=:]\s*([\d.,]+(?:\s*(?:%|ƒë·ªìng|vnd))?)',
                'confidence': 0.85,
                'type': 'allowance'
            },
            
            # 7. B·∫£o hi·ªÉm
            {
                'name': 'B·∫£o hi·ªÉm',
                'pattern': r'(b·∫£o\s*hi·ªÉm[^=:]{5,50})\s*[=:]\s*([\d.,]+\s*%)',
                'confidence': 0.9,
                'type': 'insurance_rate'
            },
            
            # 8. L·ªá ph√≠
            {
                'name': 'L·ªá ph√≠',
                'pattern': r'(l·ªá\s*ph√≠[^:=]{5,60})\s*[=:]\s*([\d.,]+\s*(?:ƒë·ªìng|vnd))',
                'confidence': 0.85,
                'type': 'fee'
            },
            
            # 9. Kho·∫£ng ti·ªÅn
            {
                'name': 'Kho·∫£ng ti·ªÅn',
                'pattern': r't·ª´\s*([\d.,]+)\s*(?:ƒë·∫øn|t·ªõi)\s*([\d.,]+)\s*(ƒë·ªìng|vnd|tri·ªáu|t·ª∑)',
                'confidence': 0.8,
                'type': 'money_range'
            },
            
            # 10. Gi·ªõi h·∫°n ti·ªÅn
            {
                'name': 'Gi·ªõi h·∫°n ti·ªÅn',
                'pattern': r'(kh√¥ng\s*(?:qu√°|v∆∞·ª£t\s*qu√°|l·ªõn\s*h∆°n|nh·ªè\s*h∆°n))\s*([\d.,]+)\s*(ƒë·ªìng|vnd|tri·ªáu|t·ª∑)',
                'confidence': 0.75,
                'type': 'money_limit'
            },
            
            # 11. Ph√©p t√≠nh ƒë∆°n gi·∫£n
            {
                'name': 'Ph√©p t√≠nh',
                'pattern': r'([\d.,]+(?:\s*(?:ƒë·ªìng|%|tri·ªáu|t·ª∑))?)\s*([+\-√ó*/])\s*([\d.,]+(?:\s*(?:ƒë·ªìng|%|tri·ªáu|t·ª∑))?)',
                'confidence': 0.7,
                'type': 'calculation'
            },
            
            # 12. C√¥ng th·ª©c trong ngo·∫∑c
            {
                'name': 'C√¥ng th·ª©c trong ngo·∫∑c',
                'pattern': r'\(([^)]{15,100}(?:[\d.,]+(?:\s*(?:%|ƒë·ªìng|vnd))?[^)]*){1,})\)',
                'confidence': 0.6,
                'type': 'parenthetical'
            },
            
            # 13. H·ªá s·ªë
            {
                'name': 'H·ªá s·ªë',
                'pattern': r'(h·ªá\s*s·ªë[^=:]{5,50})\s*[=:]\s*([\d.,]+)',
                'confidence': 0.8,
                'type': 'coefficient'
            },
            
            # 14. Gi·∫£m tr·ª´
            {
                'name': 'Gi·∫£m tr·ª´',
                'pattern': r'(gi·∫£m\s*tr·ª´[^=:]{5,50})\s*[=:]\s*([\d.,]+\s*(?:ƒë·ªìng|vnd)(?:/(?:th√°ng|nƒÉm))?)',
                'confidence': 0.9,
                'type': 'deduction'
            },
            
            # 15. M·ª©c ph·∫°t
            {
                'name': 'M·ª©c ph·∫°t',
                'pattern': r'((?:m·ª©c\s*)?ph·∫°t[^=:]{5,50})\s*[=:]\s*([\d.,]+(?:\s*(?:ƒë·ªìng|vnd|%|l·∫ßn))?)',
                'confidence': 0.85,
                'type': 'penalty'
            }
        ]
        
        # T·ª´ kh√≥a lo·∫°i tr·ª´ m·∫°nh
        self.strong_exclude = [
            'ƒëi·ªÅu', 'kho·∫£n', 'm·ª•c', 'ch∆∞∆°ng', 'ph·ª• l·ª•c', 'ph·∫ßn',
            'trang', 't·ªù', 's·ªë vƒÉn b·∫£n', 'quy·∫øt ƒë·ªãnh s·ªë', 'th√¥ng t∆∞ s·ªë',
            'website', 'email', 'http', 'www', 'javascript', 'function',
            'ng√†y', 'th√°ng', 'nƒÉm', 'gi·ªù', 'ph√∫t'
        ]
        
        # T·ª´ kh√≥a t√≠ch c·ª±c (tƒÉng ƒëi·ªÉm)
        self.positive_keywords = [
            't√≠nh', 'b·∫±ng', 'ƒë∆∞·ª£c x√°c ƒë·ªãnh', 'theo c√¥ng th·ª©c',
            '√°p d·ª•ng', 'quy ƒë·ªãnh', 'm·ª©c', 't·ª∑ l·ªá', 'thu·∫ø', 'ph√≠',
            'l∆∞∆°ng', 'ph·ª• c·∫•p', 'b·∫£o hi·ªÉm', 'gi·∫£m tr·ª´'
        ]
    
    def clean_text(self, text: str) -> str:
        """L√†m s·∫°ch text n√¢ng cao"""
        if not text:
            return ""
        
        # Remove HTML tags
        text = re.sub(r'<[^>]+>', ' ', text)
        
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove special characters but keep Vietnamese and math symbols
        text = re.sub(r'[^\w\s\d.,+\-√ó*/√∑=%:()√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒëƒê]', ' ', text)
        
        # Normalize Vietnamese currency
        text = re.sub(r'(?:VND|vnƒë|VNƒê)', 'ƒë·ªìng', text, flags=re.IGNORECASE)
        
        return text.strip()
    
    def calculate_confidence_score(self, match_text: str, base_confidence: float) -> float:
        """T√≠nh ƒëi·ªÉm confidence d·ª±a tr√™n n·ªôi dung"""
        score = base_confidence
        match_lower = match_text.lower()
        
        # TƒÉng ƒëi·ªÉm n·∫øu c√≥ t·ª´ kh√≥a t√≠ch c·ª±c
        positive_count = sum(1 for keyword in self.positive_keywords if keyword in match_lower)
        score += positive_count * 0.05
        
        # Gi·∫£m ƒëi·ªÉm n·∫øu c√≥ t·ª´ kh√≥a lo·∫°i tr·ª´
        negative_count = sum(1 for keyword in self.strong_exclude if keyword in match_lower)
        score -= negative_count * 0.1
        
        # TƒÉng ƒëi·ªÉm n·∫øu c√≥ s·ªë v√† ƒë∆°n v·ªã
        if re.search(r'[\d.,]+\s*(?:ƒë·ªìng|%|tri·ªáu|t·ª∑)', match_text):
            score += 0.1
        
        # TƒÉng ƒëi·ªÉm n·∫øu c√≥ to√°n t·ª≠
        if any(op in match_text for op in ['=', '√ó', '*', '+', '-', '/']):
            score += 0.05
        
        return min(1.0, max(0.0, score))
    
    def is_valid_match(self, match_text: str, pattern_info: Dict) -> bool:
        """Ki·ªÉm tra t√≠nh h·ª£p l·ªá n√¢ng cao"""
        if not match_text or len(match_text.strip()) < 8:
            return False
        
        match_lower = match_text.lower()
        
        # Lo·∫°i b·ªè c√°c tr∆∞·ªùng h·ª£p r√µ r√†ng kh√¥ng ph·∫£i c√¥ng th·ª©c
        for exclude in self.strong_exclude:
            if exclude in match_lower:
                return False
        
        # Lo·∫°i b·ªè ng√†y th√°ng
        if re.search(r'\d{1,2}/\d{1,2}/\d{4}', match_text):
            return False
        
        # Lo·∫°i b·ªè s·ªë vƒÉn b·∫£n
        if re.search(r'\d+/\d+/[A-Z-]+', match_text):
            return False
        
        # Ph·∫£i c√≥ s·ªë
        if not re.search(r'\d', match_text):
            return False
        
        # ƒê·ªëi v·ªõi m·ªôt s·ªë pattern ƒë·∫∑c bi·ªát, ki·ªÉm tra th√™m
        if pattern_info['type'] in ['tax_calculation', 'salary_base', 'insurance_rate']:
            # Ph·∫£i c√≥ t·ª´ kh√≥a m·∫°nh
            required_keywords = {
                'tax_calculation': ['thu·∫ø'],
                'salary_base': ['l∆∞∆°ng'],
                'insurance_rate': ['b·∫£o hi·ªÉm']
            }
            
            keywords = required_keywords.get(pattern_info['type'], [])
            if not any(keyword in match_lower for keyword in keywords):
                return False
        
        return True
    
    def extract_formulas_from_text(self, text: str) -> List[Dict]:
        """Tr√≠ch xu·∫•t c√¥ng th·ª©c t·ª´ text v·ªõi thu·∫≠t to√°n n√¢ng cao"""
        if not text:
            return []
        
        results = []
        clean_text = self.clean_text(text)
        
        # Chia text th√†nh c√°c ƒëo·∫°n ƒë·ªÉ x·ª≠ l√Ω
        paragraphs = [p.strip() for p in clean_text.split('\n') if p.strip()]
        
        for paragraph in paragraphs:
            if len(paragraph) < 20:  # Skip short paragraphs
                continue
            
            for pattern_info in self.patterns:
                pattern = pattern_info['pattern']
                matches = re.finditer(pattern, paragraph, re.IGNORECASE | re.MULTILINE)
                
                for match in matches:
                    match_text = match.group(0).strip()
                    
                    if self.is_valid_match(match_text, pattern_info):
                        # T√≠nh confidence score
                        confidence = self.calculate_confidence_score(match_text, pattern_info['confidence'])
                        
                        # T·∫°o t√™n c√¥ng th·ª©c
                        formula_name = self.generate_name(match, pattern_info)
                        
                        # L·∫•y ng·ªØ c·∫£nh
                        context = self.get_context(paragraph, match.start(), match.end())
                        
                        results.append({
                            'name': formula_name,
                            'formula': match_text,
                            'description': f"{pattern_info['name']} - {pattern_info['type']}",
                            'context': context,
                            'confidence': confidence,
                            'type': pattern_info['type'],
                            'groups': match.groups(),
                            'paragraph': paragraph[:200] + "..." if len(paragraph) > 200 else paragraph
                        })
        
        # Lo·∫°i b·ªè tr√πng l·∫∑p v√† s·∫Øp x·∫øp
        results = self.deduplicate_results(results)
        results.sort(key=lambda x: x['confidence'], reverse=True)
        
        return results[:20]  # Top 20 results
    
    def generate_name(self, match, pattern_info: Dict) -> str:
        """T·∫°o t√™n cho c√¥ng th·ª©c"""
        groups = match.groups()
        
        if groups and len(groups) > 0:
            first_group = groups[0].strip()
            if first_group and len(first_group) > 3:
                return first_group[:60]
        
        return pattern_info['name']
    
    def get_context(self, text: str, start: int, end: int, length: int = 150) -> str:
        """L·∫•y ng·ªØ c·∫£nh xung quanh c√¥ng th·ª©c"""
        context_start = max(0, start - length)
        context_end = min(len(text), end + length)
        context = text[context_start:context_end].strip()
        
        # Highlight formula in context
        formula = text[start:end]
        context = context.replace(formula, f"**{formula}**")
        
        return context
    
    def deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """Lo·∫°i b·ªè k·∫øt qu·∫£ tr√πng l·∫∑p n√¢ng cao"""
        seen = set()
        unique_results = []
        
        for result in results:
            # T·∫°o key d·ª±a tr√™n formula content (normalized)
            formula_key = re.sub(r'\s+', ' ', result['formula'].lower().strip())
            formula_key = re.sub(r'[^\w\d%.,+\-√ó*/√∑=:]', '', formula_key)
            
            if formula_key not in seen and len(formula_key) > 5:
                seen.add(formula_key)
                unique_results.append(result)
        
        return unique_results
    
    async def extract_from_page(self, page, url: str) -> Dict:
        """Tr√≠ch xu·∫•t c√¥ng th·ª©c t·ª´ trang web v·ªõi nhi·ªÅu ph∆∞∆°ng ph√°p"""
        try:
            # Th·ª≠ nhi·ªÅu c√°ch l·∫•y n·ªôi dung
            content_methods = [
                ("#tab1", "Tab1 content"),
                ("#aNoiDung", "NoiDung tab"),
                (".tab-content", "Tab content area"),
                (".content", "Content area"),
                ("body", "Full body content")
            ]
            
            all_content = ""
            content_sources = []
            
            # Click tab n·ªôi dung n·∫øu c√≥
            tab_selectors = ["#aNoiDung", "a[href='#tab1']", "a:has-text('N·ªôi dung')"]
            for selector in tab_selectors:
                try:
                    await page.click(selector, timeout=2000)
                    await page.wait_for_timeout(2000)
                    break
                except:
                    continue
            
            # L·∫•y n·ªôi dung t·ª´ nhi·ªÅu ngu·ªìn
            for selector, description in content_methods:
                try:
                    element = await page.query_selector(selector)
                    if element:
                        content = await element.inner_text()
                        if content and len(content) > 100:
                            all_content += f"\\n\\n--- {description} ---\\n{content}"
                            content_sources.append(description)
                except:
                    continue
            
            if not all_content:
                # Fallback: l·∫•y to√†n b·ªô text
                all_content = await page.inner_text("body")
                content_sources = ["Body fallback"]
            
            # Tr√≠ch xu·∫•t c√¥ng th·ª©c
            formulas = self.extract_formulas_from_text(all_content)
            
            return {
                'url': url,
                'content_length': len(all_content),
                'content_sources': content_sources,
                'formulas': formulas,
                'total_formulas': len(formulas),
                'extraction_method': 'final_multi_source'
            }
            
        except Exception as e:
            return {
                'url': url,
                'error': str(e),
                'formulas': [],
                'total_formulas': 0
            }

async def main():
    if len(sys.argv) < 2:
        print("Usage: python final_formula_extractor.py <input_file> [output_file]")
        print("Example: python final_formula_extractor.py data/real_formula_links.json")
        sys.exit(1)
    
    input_file = Path(sys.argv[1])
    output_file = Path(sys.argv[2]) if len(sys.argv) >= 3 else input_file.parent / f"{input_file.stem}_final.json"
    
    # Load links
    links = json.loads(input_file.read_text(encoding="utf-8"))
    print(f"Final Formula Extraction - Processing {len(links)} documents")
    
    extractor = FinalFormulaExtractor()
    results = []
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        
        # Load cookies if available
        context_options = {}
        cookies_path = Path("data/cookies.json")
        if cookies_path.exists():
            context_options["storage_state"] = str(cookies_path)
        
        context = await browser.new_context(**context_options)
        
        for idx, item in enumerate(links, 1):
            url = item.get("Url") or item.get("url", "")
            title = item.get("T√™n vƒÉn b·∫£n") or item.get("title", "")
            
            print(f"[{idx}/{len(links)}] {title[:60]}...")
            
            page = await context.new_page()
            try:
                await page.goto(url, wait_until="domcontentloaded", timeout=30000)
                await page.wait_for_timeout(3000)
                
                result = await extractor.extract_from_page(page, url)
                result["stt"] = idx
                result["title"] = title
                results.append(result)
                
                formulas_count = result.get("total_formulas", 0)
                if formulas_count > 0:
                    print(f"  ‚úÖ Found {formulas_count} formulas")
                    # Show top formulas
                    for i, formula in enumerate(result["formulas"][:3], 1):
                        print(f"    {i}. [{formula['confidence']:.2f}] {formula['name'][:50]}")
                        print(f"       {formula['formula'][:80]}...")
                else:
                    print(f"  ‚ùå No formulas found")
                    if result.get('error'):
                        print(f"      Error: {result['error']}")
                
            except Exception as e:
                print(f"  ‚ùå Error: {e}")
                results.append({
                    "stt": idx,
                    "url": url,
                    "title": title,
                    "error": str(e),
                    "formulas": [],
                    "total_formulas": 0
                })
            finally:
                await page.close()
        
        await browser.close()
    
    # Save results
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # Generate comprehensive summary
    total_formulas = sum(r.get("total_formulas", 0) for r in results)
    successful = len([r for r in results if r.get("total_formulas", 0) > 0])
    failed = len(results) - successful
    
    print(f"\\n{'='*80}")
    print(f"üéØ FINAL FORMULA EXTRACTION COMPLETE")
    print(f"{'='*80}")
    print(f"üìä Documents processed: {len(results)}")
    print(f"‚úÖ Successful extractions: {successful} ({successful/len(results)*100:.1f}%)")
    print(f"‚ùå Failed extractions: {failed}")
    print(f"üî¢ Total formulas found: {total_formulas}")
    print(f"üìà Average formulas per document: {total_formulas/len(results):.1f}")
    print(f"üíæ Output saved to: {output_file}")
    
    # Analyze by formula type
    type_counts = {}
    all_formulas = []
    
    for result in results:
        for formula in result.get("formulas", []):
            formula_type = formula.get("type", "unknown")
            type_counts[formula_type] = type_counts.get(formula_type, 0) + 1
            formula["source_title"] = result.get("title", "")
            all_formulas.append(formula)
    
    if type_counts:
        print(f"\\nüìã FORMULA TYPES FOUND:")
        for formula_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"   {formula_type}: {count} formulas")
    
    # Show top formulas by confidence
    if all_formulas:
        print(f"\\nüèÜ TOP FORMULAS BY CONFIDENCE:")
        all_formulas.sort(key=lambda x: x.get("confidence", 0), reverse=True)
        for i, formula in enumerate(all_formulas[:10], 1):
            print(f"{i:2d}. [{formula.get('confidence', 0):.2f}] {formula['name'][:50]}")
            print(f"     Formula: {formula['formula'][:100]}...")
            print(f"     Type: {formula.get('type', 'unknown')}")
            print(f"     Source: {formula['source_title'][:50]}...")
            print()

if __name__ == "__main__":
    asyncio.run(main())