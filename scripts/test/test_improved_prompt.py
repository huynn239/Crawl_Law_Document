#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Test v·ªõi prompt c·∫£i ti·∫øn"""
import asyncio
import os
from crawl4ai import AsyncWebCrawler, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
import sys

if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

async def test_improved_prompt():
    """Test v·ªõi prompt ƒë∆°n gi·∫£n h∆°n"""
    
    url = "https://thuvienphapluat.vn/van-ban/Giao-duc/Thong-tu-21-2025-TT-BGDDT-che-do-tra-tien-luong-day-them-gio-nha-giao-trong-cac-co-so-giao-duc-cong-lap-673797.aspx"
    
    # Prompt ƒë∆°n gi·∫£n
    simple_prompt = """
    T√¨m t·∫•t c·∫£ c√¥ng th·ª©c t√≠nh l∆∞∆°ng, ti·ªÅn, t·ª∑ l·ªá trong vƒÉn b·∫£n.
    
    T√¨m:
    - M·ª©c l∆∞∆°ng: X ƒë·ªìng
    - T·ª∑ l·ªá: X%
    - ƒê·ªãnh m·ª©c: X ti·∫øt
    - Kh√¥ng qu√° X ti·∫øt/ƒë·ªìng
    
    Tr·∫£ v·ªÅ JSON:
    {
      "formulas": [
        {
          "name": "t√™n c√¥ng th·ª©c",
          "formula": "c√¥ng th·ª©c ƒë·∫ßy ƒë·ªß",
          "description": "m√¥ t·∫£",
          "context": "ng·ªØ c·∫£nh"
        }
      ],
      "total_formulas": s·ªë_l∆∞·ª£ng
    }
    """
    
    async with AsyncWebCrawler(verbose=True) as crawler:
        # Th·ª≠ v·ªõi OpenAI tr∆∞·ªõc
        if os.getenv("OPENAI_API_KEY"):
            llm_config = LLMConfig(
                provider="openai/gpt-4o-mini",
                api_token=os.getenv("OPENAI_API_KEY")
            )
            print("Using OpenAI GPT-4o-mini")
        else:
            # Fallback to Ollama
            llm_config = LLMConfig(
                provider="ollama/qwen:7b",
                api_token=None,
                base_url="http://localhost:11434"
            )
            print("Using Ollama qwen:7b")
        
        extraction_strategy = LLMExtractionStrategy(
            llm_config=llm_config,
            extraction_type="json",
            instruction=simple_prompt
        )
        
        try:
            result = await crawler.arun(
                url=url,
                extraction_strategy=extraction_strategy,
                bypass_cache=True,
                js_code="""
                const tab1 = document.querySelector('#aNoiDung') || document.querySelector('a[href="#tab1"]');
                if (tab1) {
                    tab1.click();
                    await new Promise(resolve => setTimeout(resolve, 2000));
                }
                """,
                wait_for="networkidle"
            )
            
            if result.success:
                print("‚úÖ SUCCESS!")
                print(f"Content length: {len(result.cleaned_html)}")
                
                extracted_data = result.extracted_content
                print(f"LLM response type: {type(extracted_data)}")
                print(f"LLM response: {extracted_data}")
                
                if extracted_data:
                    # Th·ª≠ parse JSON
                    import json
                    try:
                        if isinstance(extracted_data, str):
                            data = json.loads(extracted_data)
                        else:
                            data = extracted_data
                        
                        formulas = data.get("formulas", [])
                        print(f"\nüéØ Found {len(formulas)} formulas:")
                        
                        for i, formula in enumerate(formulas, 1):
                            print(f"\n{i}. {formula.get('name', 'N/A')}")
                            print(f"   Formula: {formula.get('formula', 'N/A')}")
                            print(f"   Description: {formula.get('description', 'N/A')}")
                            
                    except Exception as e:
                        print(f"‚ùå JSON parse error: {e}")
                        print("Raw response:", str(extracted_data)[:500])
                else:
                    print("‚ùå No data extracted")
                    
                    # Fallback: show sample content
                    content = result.cleaned_html
                    lines = content.split('\n')
                    salary_lines = [line.strip() for line in lines 
                                  if line.strip() and len(line.strip()) > 20 
                                  and any(word in line.lower() for word in ['l∆∞∆°ng', 'ti·ªÅn', 'm·ª©c', 't·ª∑ l·ªá', 'ti·∫øt'])
                                  and any(char.isdigit() for char in line)]
                    
                    print(f"\nüìã Sample salary-related content ({len(salary_lines)} lines):")
                    for i, line in enumerate(salary_lines[:5], 1):
                        print(f"{i}. {line[:100]}...")
                        
            else:
                print(f"‚ùå Crawl failed: {result.error_message}")
                
        except Exception as e:
            print(f"‚ùå Error: {e}")

if __name__ == "__main__":
    asyncio.run(test_improved_prompt())